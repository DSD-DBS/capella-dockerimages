{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#welcome","title":"Welcome","text":"<p>Welcome to the MBSE Docker images repository. Initially started for our Capella Collaboration Manager to run Capella in a browser, we now offer a variety of Docker images to automate processes in the MBSE context.</p>"},{"location":"#use-prebuilt-images-from-github-packages","title":"Use prebuilt images from Github packages","text":"<p>For license reasons, we are only able to provide the following prebuilt public images at this time:</p> <ul> <li><code>base</code></li> <li><code>capella/base</code> (without dropins or plugins)</li> </ul> <p>If you need another image, please follow the <code>Build images locally</code> or <code>Build images in a CI/CD environment</code> instructions.</p>"},{"location":"#build-images-locally","title":"Build images locally","text":"<p>To get started, please clone this repository and include all submodules:</p> <pre><code>git clone https://github.com/DSD-DBS/capella-dockerimages.git\n</code></pre>"},{"location":"#build-images-using-our-cli","title":"Build images using our CLI","text":"<p>You can use our command-line interface to build, run and debug our Docker images.</p> <p>Info</p> <p>For each image, please execute the steps described in the preparation section in the documentation before building the image.</p> <p>To install the CLI, install the uv package manager and run the following command:</p> <pre><code>uv tool install -e ./cli\n</code></pre> <p>Then, just run the following command:</p> <pre><code>cdi --help\n</code></pre>"},{"location":"#build-images-manually-with-docker","title":"Build images manually with Docker","text":"<p>It's important to strictly follow the sequence. Several Docker images depend on each other. The full dependency graph for the images looks like:</p> <pre><code>flowchart LR\n    BASE(base) --&gt; CAPELLA_BASE(capella/base)\n    BASE(base) --&gt; JUPYTER(jupyter-notebook)\n    BASE(base) --&gt; PAPYRUS_BASE(papyrus/base) --&gt; PAPYRUS_REMOTE(papyrus/remote)\n    BASE(base) --&gt; ECLIPSE_BASE(eclipse/base) --&gt; ECLIPSE_REMOTE(eclipse/remote) --&gt; ECLIPSE_REMOTE_PURE_VARIANTS(eclipse/remote/pure-variants)\n    CAPELLA_BASE(capella/base) --&gt; T4C_CLIENT_BASE(t4c/client/base)\n    CAPELLA_BASE(capella/base) --&gt; CAPELLA_REMOTE(capella/remote) --&gt; CAPELLA_REMOTE_PURE_VARIANTS(capella/remote/pure-variants)\n    T4C_CLIENT_BASE(t4c/client/base) --&gt; T4C_CLIENT_REMOTE(t4c/client/remote) --&gt; T4C_CLIENT_REMOTE_PURE_VARIANTS(t4c/client/remote/pure-variants)\n\n    style BASE fill:#b22222,color:#000000\n    style CAPELLA_BASE fill:#8feb34,color:#000000\n    style JUPYTER fill:#f5626c,color:#000000\n    style PAPYRUS_BASE fill:#5f9ea0,color:#000000\n    style ECLIPSE_BASE fill:#fafad2,color:#000000\n    style T4C_CLIENT_BASE fill:#1e90ff,color:#000000\n\n    style CAPELLA_REMOTE fill:#228b22,color:#000000\n    style T4C_CLIENT_REMOTE fill:#228b22,color:#000000\n    style ECLIPSE_REMOTE fill:#228b22,color:#000000\n    style PAPYRUS_REMOTE fill:#228b22,color:#000000\n\n    style CAPELLA_REMOTE_PURE_VARIANTS fill:#62f5f2,color:#000000\n    style T4C_CLIENT_REMOTE_PURE_VARIANTS fill:#62f5f2,color:#000000\n    style ECLIPSE_REMOTE_PURE_VARIANTS fill:#62f5f2,color:#000000\n</code></pre> <p>Each highlighted color indicates the Dockerfile which is used to build the image:</p> <p> Base Capella Base Eclipse Base Papyrus Base T4C Client Base Remote pure::variants Jupyter notebook </p> <p>Make sure that all <code>docker</code> commands are executed in the root directory of the repository.</p> <p>For each image, you'll find documentation how to build &amp; run the image manually.</p>"},{"location":"#build-images-in-a-cicd-environment","title":"Build images in a CI/CD environment","text":"<p>We provide a Gitlab CI/CD template to build the images in CI/CD environment. Please find the instructions here.</p>"},{"location":"base/","title":"Base","text":""},{"location":"base/#base-image","title":"Base image","text":"<p>Info</p> <p>The Docker image name for this image is <code>base</code></p> <p>Our base image updates all <code>apt-get</code> packages and installs the following packages:</p> <ul> <li><code>python3</code></li> <li><code>python3-pip</code></li> </ul> <p>Also, we create a custom user <code>techuser</code>. This user will always be used to run the containers and allows to assign a custom <code>UID</code>. This can make sense, if you want to deploy the containers in a K8s cluster and your company has some security restrictions (e.g. specific <code>UID</code> ranges).</p> <p>Feel free to modify this image to your specific needs. You are able to set proxies, custom registry URLs, your timezone, CA certificates and any other stuff.</p> <p>The following environment variable can be set in all images:</p> <ul> <li><code>WORKSPACE_DIR</code>: The directory applications (Eclipse, Capella, Jupyter) will   use as workspace. The workspace directory shall be a subdirectory of   <code>/workspace</code> or <code>/home/techuser</code>.</li> </ul>"},{"location":"base/#use-the-prebuilt-image","title":"Use the prebuilt image","text":"<pre><code>docker run -it ghcr.io/dsd-dbs/capella-dockerimages/base:$CAPELLA_DOCKER_IMAGES_REVISION\n</code></pre> <p>where <code>$CAPELLA_DOCKER_IMAGES_REVISION</code> is the tag or branch of this repository. In case of branches, replace all characters matching the regex <code>[^a-zA-Z0-9.]</code> with <code>-</code>.</p>"},{"location":"base/#build-it-yourself","title":"Build it yourself","text":""},{"location":"base/#build-it-manually-with-docker","title":"Build it manually with Docker","text":"<p>To build the base image, please run:</p> <pre><code>docker build -t base base\n</code></pre> <p>Important: If your company has a specific base image with all company configurations, of course, it can also be used:</p> <pre><code>docker build -t base --build-arg BASE_IMAGE=$CUSTOM_IMAGE base\n</code></pre> <p>Make sure that your <code>$CUSTOM_IMAGE</code> is a Linux image that has the common tools installed and uses the <code>apt</code> / <code>apt-get</code> package manager. If this is not the case, the image can not be used. Our images were tested with the image <code>debian:bookworm</code>.</p> <p>If you like to set a custom <code>UID</code> for the user <code>techuser</code>, you can run:</p> <pre><code>docker build -t base --build-arg UID=1001 base\n</code></pre>"},{"location":"pure-variants/","title":"pure::variants","text":""},{"location":"pure-variants/#purevariants","title":"pure::variants","text":"<p>Info</p> <p>The Docker image name for this image is <code>&lt;base&gt;/pure-variants</code> or <code>&lt;base&gt;/pure-variants</code> where <code>&lt;base&gt;</code> is one of the following options:</p> <ul> <li><code>capella/remote</code></li> <li><code>t4c/client/remote</code></li> <li><code>eclipse/remote</code></li> </ul> <p>for remote images or</p> <ul> <li><code>capella/base</code></li> <li><code>t4c/client/base</code></li> <li><code>eclipse/base</code></li> </ul> <p>for local images without the remote feature.</p> <p>As part of this Docker image, <code>pure::variants</code> is installed into the Eclipse software of the base image. In addition, it has some basic configuration support, e.g., for setting the license server automatically during runtime.</p> <p>If the base image is based on Capella, the <code>pure::variants</code> Capella plugin is installed in addition.</p>"},{"location":"pure-variants/#build-it-yourself","title":"Build it yourself","text":""},{"location":"pure-variants/#preparation","title":"Preparation","text":""},{"location":"pure-variants/#download-the-purevariants-archive","title":"Download the <code>pure::variants</code> archive","text":"<ol> <li> <p>Download the pure::variants updateSite from the pure::variants download     site: https://www.pure-systems.com/pvde-update/ (access restricted,     license needed). The version on the     public website is not sufficient     (it's missing important plugins).</p> <p>Please select: \"pure::variants Archived Update Site with all Extensions\" for Linux (Tux).</p> </li> <li> <p>Place the zip-file into <code>pure-variants/versions/$PURE_VARIANTS_VERSION</code>.     <code>$PURE_VARIANTS_VERSION</code> is the sematic version of pure::variants, e.g.     <code>6.0.0</code>.</p> </li> </ol>"},{"location":"pure-variants/#build-it-manually-with-docker","title":"Build it manually with Docker","text":"<ol> <li>Start the Docker build:</li> </ol> <pre><code>docker build -t &lt;base&gt;/pure-variants \\\n     --build-arg BASE_IMAGE=&lt;base&gt; \\\n     --build-arg PURE_VARIANTS_VERSION=$PURE_VARIANTS_VERSION \\\n    pure-variants\n</code></pre> <p>where <code>&lt;base&gt;</code> is one of the options listed in the infobox above.</p>"},{"location":"pure-variants/#run-the-container","title":"Run the container","text":"<p>To run the <code>pure-variants</code> images, please follow the instructions to run the Capella base or T4C client base image, but consider the following differences:</p> <ul> <li>Add the environment variable <code>$PURE_VARIANTS_LICENSE_SERVER</code> to the   <code>docker run</code> command. The value is the same as set in the Eclipse GUI when   running a normal installation, e.g. <code>http://localhost:8080</code>.</li> <li>Bind the directory containing the <code>license.lic</code> file to   <code>/inputs/pure-variants/</code> inside the container.   <pre><code>docker run -d \\\n    -p $RDP_EXTERNAL_PORT:3389 \\\n    -e RMT_PASSWORD=$RMT_PASSWORD \\\n    $BASE_IMAGE/remote/pure-variants\n</code></pre></li> </ul>"},{"location":"remote/","title":"Remote","text":""},{"location":"remote/#remote-image","title":"Remote image","text":"<p>The remote images allow to extend the</p> <ul> <li>Capella base image (<code>capella/base</code>)</li> <li>T4C base image (<code>t4c/client/base</code>)</li> <li>Pure::variants image (<code>t4c/client/pure-variants</code>)</li> </ul> <p>with a RDP or XPRA server and a metrics endpoint to measure the container activity.</p> <p>It is a basic Linux server with a Openbox installation.</p>"},{"location":"remote/#build-it-yourself","title":"Build it yourself","text":""},{"location":"remote/#preparation","title":"Preparation","text":""},{"location":"remote/#optional-customize-openbox","title":"Optional: Customize Openbox","text":"<p>Openbox is only used for the connection method RDP.</p> <p>Feel free to adjust the configurations <code>remote/rc.xml</code> and <code>remote/menu.xml</code> to satisfy custom Openbox configuration needs.</p>"},{"location":"remote/#build-it-manually-with-docker","title":"Build it manually with Docker","text":"<pre><code>docker build -t $BASE_IMAGE/remote remote --build-arg BASE_IMAGE=$BASE_IMAGE\n</code></pre> <p>where <code>$BASE_IMAGE</code> is <code>capella/base</code>, <code>t4c/client/base</code> or <code>t4c/client/pure-variants</code> (depends on the image you'd like to extend with remote functionality)</p>"},{"location":"remote/#run-the-container","title":"Run the container","text":"<p>Replace the followings variables:</p> <ul> <li><code>$BASE_IMAGE</code> with <code>capella/base</code>, <code>t4c/client/base</code> or   <code>t4c/client/pure-variants</code>. Please check the individual section   <code>In a remote container (RDP)</code> of the individual base image documentation   pages for additional configuration options.</li> </ul> Connect via RDPConnect via XPRA <p>The container image contains a <code>xrdp</code> server. To use RDP to connect to the container, run the container with the following command:</p> <pre><code>docker run -d \\\n    -p $RDP_EXTERNAL_PORT:3389 \\\n    -e CONNECTION_METHOD=xrdp \\\n    -e RMT_PASSWORD=$RMT_PASSWORD \\\n    $BASE_IMAGE/remote\n</code></pre> <p>Replace <code>$RDP_EXTERNAL_PORT</code> with the external port that the RDP server should listen on (usually <code>3389</code>).</p> <p>Also replace <code>$RMT_PASSWORD</code> with the password for remote connections (for the login via RDP). The value has to be at least 8 characters long.</p> <p>After starting the container, you should be able to connect to <code>localhost:$RDP_EXTERNAL_PORT</code> with your preferred RDP Client.</p> <p>For the login use the followings credentials:</p> <ul> <li>Username: <code>techuser</code></li> <li>Password: <code>$RMT_PASSWORD</code></li> </ul> <p>The screen size is set every time the connection is established. Depending on your RDP client, you will also be able to set the preferred screen size in the settings.</p> <p>By default, Remmina (RDP client for Linux) starts in a tiny window. To fix that, you can set \"Use client resolution\" instead of \"Use initial window size\" in the remote connection profile.</p> <p>The container image contains a <code>xpra-html5</code> server. To use XPRA via HTML5 to connect to the container, run the container with the following command:</p> <pre><code>docker run -d \\\n    -p $XPRA_PORT:10000 \\\n    -e CONNECTION_METHOD=xpra \\\n    -e XPRA_SUBPATH=\"/\" \\\n    $BASE_IMAGE/remote\n</code></pre> <p>Authentication</p> <p>Since version v1.19.0, xpra based containers are exposed without authentication. If used with the Capella Collaboration Manager &gt;= v3.1.0, authentication is handled via session pre-authentication automatically. In other cases, you have to implement your own authentication mechanism.</p> <p>Embedding in iframes</p> <p>To embed the XPRA session in an iframe, you have to set a custom Content Security Policy. You can pass the environment variable <code>XPRA_CSP_ORIGIN_HOST</code> to the hostname of the website you'd like to embed the XPRA session in. If you want to embed the XPRA session in an iframe on <code>example.com</code>, set <code>XPRA_CSP_ORIGIN_HOST</code> to <code>https://example.com</code>.</p> <p>Set the <code>XPRA_SUBPATH</code> to the subpath that <code>xpra</code> should serve on. If you want to have it running on <code>/xpra</code>, set <code>XPRA_SUBPATH</code> to <code>/xpra</code>.</p> <p>Then, open a browser and connect to: <pre><code>http://localhost:${XPRA_PORT}${XPRA_SUBPATH}/?floating_menu=0\n</code></pre></p> <p>More configuration options can be passed as query parameters. See the xpra-html5 documentation for more information.</p>"},{"location":"capella/base/","title":"Base","text":""},{"location":"capella/base/#capella-base","title":"Capella base","text":"<p>Info</p> <p>The Docker image name for this image is <code>capella/base</code></p> <p>The Capella base image installs a selected Capella client version. The Capella client can be downloaded and can optionally be customised prior to building the Docker image or can be downloaded automatically in the Docker image.</p> <p>The images are meant to have a containerised Capella (with or without a Team for Capella client) that can be run headless (as command line interface).</p> <p>Info</p> <p>The functionality for running capella as a command-line app used to be part of the <code>capella/cli</code> image. An image with this name is no longer built. Use <code>capella/base</code> instead.</p>"},{"location":"capella/base/#use-the-prebuilt-image","title":"Use the prebuilt image","text":"<pre><code>docker run ghcr.io/dsd-dbs/capella-dockerimages/capella/base:$TAG\n</code></pre> <p>where <code>$TAG</code> is the Docker tag. For more information, have a look at our tagging schema.</p> <p>Please check the <code>Run the container</code> section for more information about the usage.</p>"},{"location":"capella/base/#build-it-yourself","title":"Build it yourself","text":""},{"location":"capella/base/#preparation","title":"Preparation","text":""},{"location":"capella/base/#optional-download-capella-manually","title":"Optional: Download Capella manually","text":"<p>Download a Capella Linux binary <code>zip</code> or <code>tar.gz</code> archive. You can get a release directly from Eclipse. Visit https://github.com/eclipse/capella/releases, select a version and follow the hyperlink labelled <code>Product</code> to find a binary release for Linux.</p> <p>Place the downloaded archive in the subdirectory <code>capella/versions/$CAPELLA_VERSION/$ARCHITECTURE</code> of the present repository and ensure that the end result is either</p> <ul> <li><code>capella/versions/$CAPELLA_VERSION/$ARCHITECTURE/capella.tar.gz</code> or</li> <li><code>capella/versions/$CAPELLA_VERSION/$ARCHITECTURE/capella.zip</code>.</li> </ul> <p>Check that the archive has a structure similar to the following coming with a top level directory named <code>capella</code> and several sub directories and files in it.</p> <p>For Capella 5.0.0 the structure is illustrated below:</p> <pre><code>$ tree -L 1 capella\ncapella\n\u251c\u2500\u2500 artifacts.xml\n\u251c\u2500\u2500 capella\n\u251c\u2500\u2500 capella.ini\n\u251c\u2500\u2500 configuration\n\u251c\u2500\u2500 dropins\n\u251c\u2500\u2500 epl-v10.html\n\u251c\u2500\u2500 features\n\u251c\u2500\u2500 jre\n\u251c\u2500\u2500 notice.html\n\u251c\u2500\u2500 p2\n\u251c\u2500\u2500 plugins\n\u2514\u2500\u2500 readme\n</code></pre>"},{"location":"capella/base/#install-dropins","title":"Install dropins","text":""},{"location":"capella/base/#automatic-installation","title":"Automatic installation","text":"<p>The image builder can automatically download dropins for Capella and inject them into the Capella client.</p> <p>You have to pass a comma-separated list of dropin names as <code>CAPELLA_DROPINS</code> build argument to the <code>docker build</code> command:</p> <pre><code>--build-arg CAPELLA_DROPINS=\"ModelsImporter,CapellaXHTMLDocGen,DiagramStyler,PVMT,Filtering,Requirements,SubsystemTransition\"\n</code></pre> <p>Supported dropins are:</p> <ul> <li>CapellaXHTMLDocGen</li> <li>DiagramStyler</li> <li>PVMT</li> <li>Filtering</li> <li>Requirements</li> <li>SubsystemTransition</li> <li>TextualEditor</li> </ul> <p>The dropins are registered in the <code>capella/versions/$CAPELLA_VERSION/dropins.yml</code> file. If you're missing a dropin in the list, feel free to open a PR.</p>"},{"location":"capella/base/#manual-installation","title":"Manual installation","text":"<p>If you want to install dropins manually, you can place the dropins in the <code>capella/versions/$CAPELLA_VERSION/dropins</code> directory. The dropins will be copied into the <code>dropins</code> directory of the Capella client without any further processing.</p> <p>If you only have the updateSite and not the dropin, you can create a file <code>dropins.overwrites.yml</code> next to the <code>dropins.yml</code> in <code>capella/versions/$CAPELLA_VERSIONS</code> with the following content:</p> <pre><code>dropins:\n  NameOfTheDropin:\n    type: updateSite\n    eclipseRepository:\n      - &lt;filename&gt;.zip # (1)!\n      - 'https://download.eclipse.org/releases/2023-03' # (2)!\n    installIU: # (3)!\n      - org.eclipse.xxx.feature.feature.group\n    tag: \"FeaturePatch\" # (4)!\n    profile: \"DefaultProfile\" # (5)!\n    autouse: false # (6)!\n    proxy: null # (7)!\n    disable_mirrors: false # (8)!\n</code></pre> <ol> <li>Place the file in the <code>capella/versions/$CAPELLA_VERSION/patches</code> directory and specify the filename here. Must be a zip file.</li> <li>Specify additional registries like the Eclipse registry to fetch dependencies from.</li> <li>Add all IDs of installable units that you want to install. You can see those while installing the updateSite via the Capella UI.</li> <li>Optional tag if required. Remove the line if no tag is required.</li> <li>Optional profile to use. Remove the line if no profile is required. If transitioning from <code>patch_info.csv</code>, use the profile <code>DefaultProfile</code>.</li> <li>If <code>autouse</code> is enabled, the dropin will be installed independently of the <code>CAPELLA_DROPINS</code> build argument.</li> <li>Optional proxy to fetch external registries. If not set, no proxy will be used. To use the proxy of the environment, set it to <code>{proxy}</code>.</li> <li>Decide if mirrors should be disabled or not. This might be needed if you run your own mirror.</li> </ol>"},{"location":"capella/base/#build-it-manually-with-docker","title":"Build it manually with Docker","text":""},{"location":"capella/base/#automatic-download-of-capella","title":"Automatic download of Capella","text":"<p>If you want to download the Capella archive automatically, use the following command. It does only work for supported Capella versions.</p> <pre><code>docker build -t capella/base capella --build-arg BUILD_TYPE=online --build-arg CAPELLA_VERSION=$CAPELLA_VERSION\n</code></pre> <p>You can further customize the mirror to download the Capella archive from. If you don't want to use the default mirror, choose another mirror from this list.</p> <p>Then, append <code>/{}</code> to the mirror URL and pass it as a build argument to the above command, e.g.:</p> <pre><code>--build-arg CAPELLA_DOWNLOAD_URL=\"https://mirror.umd.edu/eclipse/capella/core/products/releases/{}\"`\n</code></pre>"},{"location":"capella/base/#build-with-pre-downloaded-version-of-capella","title":"Build with pre-downloaded version of Capella","text":"<p>If you've downloaded the Capella archive manually before, use this command:</p> <pre><code>docker build -t capella/base capella --build-arg CAPELLA_VERSION=$CAPELLA_VERSION\n</code></pre> <p>With this method, you can customize the Capella client before running the above command:</p> <ol> <li>Extract the downloaded archive,</li> <li>Apply any modifications (e.g., installation of plugins and/ or dropins) to    it, and</li> <li>Compress the modified folder <code>capella</code> to get a <code>capella.zip</code> or    <code>capella.tar.gz</code> again.</li> </ol>"},{"location":"capella/base/#miscellaneous","title":"Miscellaneous","text":""},{"location":"capella/base/#run-the-container","title":"Run the container","text":""},{"location":"capella/base/#configuration-options","title":"Configuration Options","text":"<p>There are a few configuration options that can be passed to the container.</p>"},{"location":"capella/base/#semantic-browser-auto-refresh","title":"Semantic Browser Auto-refresh","text":"<p>One performance recommendation of the Capella team is to disable the semantic browser auto-refresh.</p> <p>The semantic browser synchronization is disabled by default in our containers. To follow a more streamlined approach, it will also be disabled if actively changed in the UI / workspace.</p> <p>To disable this behaviour and just keep the option as it is, pass the following flag to the <code>docker run</code> command:</p> <pre><code>--env CAPELLA_DISABLE_SEMANTIC_BROWSER_AUTO_REFRESH=0\n</code></pre>"},{"location":"capella/base/#locally-on-x11-systems","title":"Locally on X11 systems","text":"<p>If you don't need remote access, have a local X11 server running and just want to run Capella locally, this may be the best option for you.</p> <p>On some systems, you have to whitelist connections to the X-Server with:</p> <pre><code>xhost +local\n</code></pre> <p>It allows all local programs to connect to your X server. You can further restrict the access to the X server. Please read the documentation of <code>xhost</code> for more details.</p> <p>The container can be started with the following command. The <code>DISPLAY</code> environment has to be passed to the container.</p> <pre><code>docker run -d \\\n    -v /tmp/.X11-unix:/tmp/.X11-unix \\\n    -e DISPLAY=$(DISPLAY) \\\n    capella/base\n</code></pre> <p>Capella should start after a few seconds.</p>"},{"location":"capella/base/#in-a-remote-container","title":"In a remote container","text":"<p>Please follow the instructions on the remote page. When running the image, add the following variables to the <code>docker run</code> command:</p> <pre><code>    -e AUTOSTART_CAPELLA=$AUTOSTART_CAPELLA \\\n    -e RESTART_CAPELLA=$RESTART_CAPELLA \\\n</code></pre> <p>Please replace the followings variables:</p> <ul> <li><code>AUTOSTART_CAPELLA</code> defines the autostart behaviour of Capella. When set to 1   (default), Capella will be started as soon as an RDP connection has been   established to the running container.</li> <li><code>RESTART_CAPELLA</code> defines the restart behaviour of Capella. When set to 1   (default) and when <code>AUTOSTART_CAPELLA=1</code>, Capella will be re-started as soon   as it has been exited (after clean quits as well as crashs).</li> </ul> <p>If you want to configure the JVM memory options, have a look at Eclipse memory options.</p>"},{"location":"capella/base/#example-to-export-representations-diagrams-as-svg-images","title":"Example to export representations (diagrams) as SVG images","text":"<p>Replace <code>/path/to/model</code> and <code>&lt;PROJECT_NAME&gt;</code> to pass any local Capella model. Set the project name so that it fits your Capella project name for the model as it is given in the file <code>/path/to/model/.project</code>.</p> <p>Exported diagrams will appear on the host machine at <code>/path/to/model/diagrams</code>.</p> <pre><code>docker run --rm -it \\\n  -v /path/to/model:/model \\\n  capella/base \\\n  -nosplash \\\n  -consolelog \\\n  -application org.polarsys.capella.core.commandline.core \\\n  -appid org.polarsys.capella.exportRepresentations \\\n  -data /workspace \\\n  -import /model \\\n  -input \"/all\" \\\n  -imageFormat SVG \\\n  -exportDecorations \\\n  -outputfolder /&lt;PROJECT_NAME&gt;/diagrams \\\n  -forceoutputfoldercreation\n</code></pre>"},{"location":"capella/build-from-source/","title":"Build from source","text":""},{"location":"capella/build-from-source/#build-capella-from-source","title":"Build Capella from source","text":"<p>We provide a script, which runs a Docker container to build Capella from source. Our builder script adds the aarch64 build platform for macOS and linux. Please note that Capella itself has no official support for the aarch64 architecture yet. We can not guarantee that everything works, but all of the official Capella tests passed on the generated archives.</p> <p>In addition, the builder only works for Capella version 6.0.0 for now. To start the script, please run:</p> <pre><code>CAPELLA_VERSION=6.0.0 make capella/builder\n</code></pre> <p>When the script is successful, you should see the result in <code>builder/output</code>.</p> <p>We store a cache of the maven repository in the <code>builder/m2_cache</code> directory. If you don't need the cache anymore, you can delete the directory after building.</p>"},{"location":"capella/introduction/","title":"Introduction","text":""},{"location":"capella/introduction/#capella-introduction","title":"Capella introduction","text":""},{"location":"capella/introduction/#supported-versions","title":"Supported versions","text":"<p>Currently, we support Capella versions <code>5.0.0</code>, <code>5.2.0</code>, <code>6.0.0</code>, <code>6.1.0</code> and <code>7.0.0</code>.</p>"},{"location":"capella/introduction/#supported-architectures","title":"Supported architectures","text":"<p>Currently, we support amd64 for all supported Capella version. In addition, we added support for arm64 starting with Capella <code>6.0.0</code>.</p>"},{"location":"capella/introduction/#supported-dropins","title":"Supported dropins","text":"<p>We have prebuilt images for Capella versions <code>6.1.0</code> and <code>7.0.0</code> with a pre-selected set of dropins. Available options are:</p> <ul> <li><code>without-dropins</code>: Without dropins</li> <li><code>selected-dropins</code>: With   CapellaXHTMLDocGen,   DiagramStyler,   PVMT,   Filtering,   Requirements and   SubsystemTransition (The CDO parts of the extensions are not included, but required for compatibility with TeamForCapella, see #400)</li> </ul> <p>If you need a custom set of dropins, you have two options:</p> <p>Option 1: Mount a dropins folder with additional dropins into <code>/opt/capella/dropins</code> when starting the container.</p> <p>Option 2: Build the <code>capella/base</code> Docker image manually. More information: Build it yourself</p>"},{"location":"capella/introduction/#tagging-schema-for-prebuilt-images","title":"Tagging schema for prebuilt images","text":"<p>The Capella related images are tagged using the following schema:</p> <p><code>$CAPELLA_VERSION-$DROPINS_TYPE-$CAPELLA_DOCKER_IMAGES_REVISION</code>, e.g., <code>6.0.0-selected-dropins-v1.10.2</code> for Capella version <code>6.0.0</code> with selected dropins and Capella Docker images revision <code>v1.10.2</code>.</p> <p><code>$CAPELLA_VERSION</code> is the semantic version of Capella (see supported versions above). <code>$DROPINS_TYPE</code> is the name of the set of dropins. <code>$CAPELLA_DOCKER_IMAGES_REVISION</code> can be a tag or branch of this repository. In case of branches, all characters matching the regex <code>[^a-zA-Z0-9.]</code> will be replaced with <code>-</code>.</p> <p>We don't tag images with the <code>latest</code> tag. You may want to use <code>$CAPELLA_VERSION-selected-dropins-main</code> for the latest version, but we recommend using tags for the best stability.</p>"},{"location":"capella/introduction/#tips","title":"Tips","text":"<ul> <li> <p>You can mount a Capella workspace inside the container by appending the   following to the <code>docker run</code> command:   </p> <pre><code>-v /path/to/your/local/volume:/workspace\n</code></pre> </li> </ul>"},{"location":"capella/provisioning/","title":"Provisioning","text":""},{"location":"capella/provisioning/#load-models-to-your-workspace-automatically","title":"Load models to your workspace automatically","text":"<p>Migration from <code>v1.X.X</code> to <code>v2.X.X</code> and later</p> <p>This feature replaces the read-only image of version <code>v1.X.X</code>. Before starting the new image, you have to clone the Git repositories that you've passed to the read-only image manually. The path with all repositories can then be mounted to the new image as described below.</p> <p>Technical prerequisites</p> <p>The feature relies on an Eclipse plugin that is not part of Capella. The plugin is called <code>models-from-directory-importer</code> and is available on GitHub: https://github.com/DSD-DBS/capella-addons.</p> <p>The plugin is part of all Capella based pre-built images on GitHub. If you build it manually, make sure that you follow the \"Install dropins\" instructions.</p> <p>To load models to your workspace automatically, you can mount a volume to the container.</p> <pre><code>docker run -d \\\n    -v path/to/models/on/host:/models \\\n    -e ECLIPSE_PROJECTS_TO_LOAD='[]' \\\n    capella/base\n</code></pre> <p>The <code>ECLIPSE_PROJECTS_TO_LOAD</code> environment variable is a JSON array that contains:</p> <pre><code>[\n  {\n    \"revision\": \"master\", // (1)\n    \"nature\": \"project\", // (2)\n    \"path\": \"/models/directory\", // (3)\n    \"entrypoint\": \"test.aird\" // (4)\n  }\n]\n</code></pre> <ol> <li>The revision of the Eclipse project. In case of duplicated project names,    the revision is added as suffix to the project name.</li> <li>Optional: Can be either 'project' or 'library'. Defaults to 'project'.    Ignored if the the directory provided in the <code>path</code> attribute contains a    <code>.project</code> file.</li> <li>Path to the directory where the project should be loaded from.</li> <li>Path to the aird file, starting from the directory provided in the <code>path</code>    attribute. Required if the <code>.aird</code> is not placed directly in the directory    provided as <code>path</code>. If None, the aird is searched in the path directory    without recursion.</li> </ol> <p>All additional attributes are ignored.</p> <p>You can use all images that are based on the <code>capella/base</code> image for this feature.</p>"},{"location":"capella/t4c/base/","title":"Base","text":""},{"location":"capella/t4c/base/#teamforcapella-client-base","title":"TeamForCapella client base","text":"<p>Info</p> <p>The Docker image name for this image is <code>t4c/client/base</code></p> <p>The T4C base image builds on top of the Capella base image and installs the T4C client plugins.</p>"},{"location":"capella/t4c/base/#build-it-yourself","title":"Build it yourself","text":""},{"location":"capella/t4c/base/#preparation","title":"Preparation","text":""},{"location":"capella/t4c/base/#download-teamforcapella-bundle","title":"Download TeamForCapella bundle","text":"<ol> <li> <p>Download a Team for Capella client for Linux from    https://www.obeosoft.com/en/team-for-capella-download</p> <p>Note that the T4C client version must match the version for Capella itself.</p> </li> <li> <p>Extract the downloaded archive. The extracted folder comes with a <code>.zip</code> file    containing the T4C client:</p> <pre><code>$ tree -L 2 TeamForCapella-5.0.0-linux.gtk.x86_64\nTeamForCapella-5.0.0-linux.gtk.x86_64\n\u251c\u2500\u2500 (...)\n\u2514\u2500\u2500 updateSite\n    \u2514\u2500\u2500 com.thalesgroup.mde.melody.team.license.update-5.0.0-202012091024.zip\n</code></pre> </li> <li> <p>That <code>.zip</code> file needs to be copied into the subdirectory <code>t4c/updateSite/$CAPELLA_VERSION</code>    of the present repository.</p> </li> </ol>"},{"location":"capella/t4c/base/#optional-add-feature-patches","title":"Optional: Add feature patches","text":"<p>It is possible to provide feature patches for our t4c base image that are installed after the initial installation. To install such feature patches, you have to do the following things.</p> <ol> <li>The feature patch <code>.zip</code> file needs to be copied into the subdirectory    <code>t4c/updateSite/$CAPELLA_VERSION</code> of the present repository</li> <li>You have to create the <code>patch_info.csv</code> file inside the same subdirectory if    not yet existing</li> <li>You have to add a new line to the <code>patch_info.csv</code> having the following    format:</li> </ol> <pre><code>&lt;feature patch zip file&gt;,&lt;install iu&gt;,&lt;tag&gt;\n</code></pre> <p>In case that you have one feature patch zip containing different things you    want to install you can provide multiple install iu, each with a    whitespace seperated. So in this case the <code>patch_info.csv</code> would contain a    line with the following format:</p> <pre><code>&lt;feature patch zip file&gt;,&lt;install iu 1&gt; &lt;install iu 2&gt; ... &lt;install iu n&gt;,&lt;tag&gt;\n</code></pre> <p>Please ensure that the <code>patch_info.csv</code> contains an empty line at the end otherwise the last feature patch might not be installed.</p>"},{"location":"capella/t4c/base/#build-it-manually-with-docker","title":"Build it manually with Docker","text":"<p>Build the container:</p> <pre><code>docker build -t t4c/client/base --build-arg CAPELLA_VERSION=$CAPELLA_VERSION t4c\n</code></pre>"},{"location":"capella/t4c/base/#run-the-container","title":"Run the container","text":"<p>Running the T4C client container is analogous to the Capella Base container. Please run the instructions of the Capella Base container, but add the following environment variables during the <code>docker run</code> command:</p> <pre><code>    -e T4C_LICENCE_SECRET=XXX \\\n    -e T4C_JSON='[{\"repository\": \"\", \"port\": 0, \"host\": \"\", \"instance\": \"\", \"protocol\": \"ssl\"}]' \\\n    -e T4C_SERVER_HOST=$T4C_SERVER_HOST \\\n    -e T4C_SERVER_PORT=$T4C_SERVER_PORT \\\n    -e T4C_REPOSITORIES=$T4C_REPOSITORIES \\\n    -e T4C_USERNAME=$T4C_USERNAME \\\n</code></pre> <p>Please replace the followings variables:</p> <ul> <li><code>$T4C_LICENCE_SECRET</code> with your TeamForCapella licence secret.</li> <li><code>$T4C_USERNAME</code> with the username that is suggested when connecting to t4c.</li> <li> <p>One of the two options:</p> <ul> <li> <p><code>$T4C_JSON</code> with a list of repositories with name, host, port and instance name as JSON:      <pre><code>[\n   {\n      \"repository\": \"repoCapella\",\n      \"host\": \"localhost\",\n      \"port\": 2036,\n      \"instance\": \"\", //optional, required if the repository names are not unique\n      \"protocol\": \"ssl\" //optional, defaults to ssl\n   }\n]\n</code></pre></p> <p>The environment variables <code>$T4C_SERVER_HOST</code>, <code>$T4C_SERVER_PORT</code> and <code>$T4C_REPOSITORIES</code> will be ignored.</p> </li> <li> <p>Three environment variables:</p> <ul> <li><code>$T4C_SERVER_HOST</code> with the IP-Address of your T4C server (default: <code>127.0.0.1</code>).</li> <li><code>$T4C_SERVER_PORT</code> with the port of your T4C server (default: <code>2036</code>).</li> <li><code>$T4C_REPOSITORIES</code> with a comma-seperated list of repositories. These repositories show     up as default options on connection (e.g. <code>repo1,repo2</code>).</li> </ul> </li> </ul> </li> </ul> <p>When Capella has started, you should see the T4C models in the dropdown menu of the connection dialog.</p>"},{"location":"capella/t4c/exporter/","title":"Exporter","text":""},{"location":"capella/t4c/exporter/#teamforcapella-client-exporter","title":"TeamForCapella client exporter","text":"<p>Info</p> <p>The Docker image name for this image is <code>t4c/client/base</code></p> <p>Info</p> <p>The exporter can export a model from Git to a TeamForCapella repository with the <code>merge</code>-strategy of TeamForCapella.</p> <p>The T4C client exporter image imports a model from a git repository and exports it to a T4C server.</p>"},{"location":"capella/t4c/exporter/#build-it-yourself","title":"Build it yourself","text":""},{"location":"capella/t4c/exporter/#build-it-manually-with-docker","title":"Build it manually with Docker","text":"<p>Build instructions are the same as the T4C client base image.</p>"},{"location":"capella/t4c/exporter/#run-the-container","title":"Run the container","text":"<p>Run the following command to export from Git to T4C:</p> <pre><code>docker run -d \\\n  -e GIT_REPO_URL=https://github.com/example/example.git \\\n  -e GIT_REPO_BRANCH=main \\\n  -e GIT_USERNAME=user \\\n  -e GIT_PASSWORD=password \\\n  -e T4C_REPO_HOST=localhost \\\n  -e T4C_REPO_PORT=2036 \\\n  -e T4C_REPO_NAME=repoCapella \\\n  -e T4C_PROJECT_NAME=test \\\n  -e T4C_USERNAME=user \\\n  -e T4C_PASSWORD=password \\\n  -e LOG_LEVEL=DEBUG \\\n  t4c/client/base export\n</code></pre> <p>You can find the description for these values in the run instructions of the importer.</p>"},{"location":"capella/t4c/exporter/#testing","title":"Testing","text":""},{"location":"capella/t4c/exporter/#manual-testing","title":"Manual Testing","text":"<p>For development purposes, you can test the exporter locally.</p> <p>Warning</p> <p>For the next steps, you need a running TeamForCapella server.</p> <ol> <li>Start a lightweight local Git server with the following command:    <pre><code>make run-local-git-server\n</code></pre></li> <li>Clone the sample repository:    <pre><code>git clone 'http://localhost:10001/git/git-test-repo.git'\n</code></pre></li> <li> <p>Copy the model to the newly created repository. Push the model you'd like to    export:</p> <pre><code>git add .\ngit commit -m \"Initial commit\"\ngit push\n</code></pre> </li> <li> <p>Set the <code>GIT_REPO_ENTRYPOINT</code> environment variable to the relative path from    the root of the repository to the aird file:</p> <pre><code>export GIT_REPO_ENTRYPOINT=\"path/to/your/model.aird\"\n</code></pre> </li> <li> <p>Create a new TeamForCapella repository via the REST API.</p> </li> <li> <p>Set the <code>T4C_REPO_NAME</code> environment variable to the repository name that you    chose in the previous step:</p> <pre><code>export T4C_REPO_NAME=\"repoCapella\"\n</code></pre> </li> <li> <p>Run the following command to start the exporter:     <pre><code>make run-t4c/client/exporter\n</code></pre></p> </li> <li>Connect to the repository from a Capella client to verify that the model was    exported correctly.</li> </ol>"},{"location":"capella/t4c/importer/","title":"Importer","text":""},{"location":"capella/t4c/importer/#teamforcapella-client-backup-importer","title":"TeamForCapella client backup / importer","text":"<p>Info</p> <p>The Docker image name for this image is <code>t4c/client/base</code></p> <p>Info</p> <p>The importer exports a model from a TeamForCapella repository to a Git repository.</p> <p>The T4C client backup image imports a model from a TeamForCapella server and exports it to a Git repository. It can be used as a backup solution, for example, in a scheduled job.</p>"},{"location":"capella/t4c/importer/#build-it-yourself","title":"Build it yourself","text":""},{"location":"capella/t4c/importer/#build-it-manually-with-docker","title":"Build it manually with Docker","text":"<p>Build instructions are the same as the T4C client base image.</p>"},{"location":"capella/t4c/importer/#run-the-container","title":"Run the container","text":"<p>Please run the following command to run the backup from T4C to Git:</p> <pre><code>docker run -d \\\n  -e GIT_REPO_URL=https://github.com/example/example.git \\\n  -e GIT_REPO_BRANCH=main \\\n  -e GIT_USERNAME=user \\\n  -e GIT_PASSWORD=password \\\n  -e T4C_REPO_HOST=localhost \\\n  -e T4C_REPO_PORT=2036 \\\n  -e T4C_CDO_PORT=12036 \\\n  -e T4C_REPO_NAME=repoCapella \\\n  -e T4C_PROJECT_NAME=test \\\n  -e T4C_USERNAME=user \\\n  -e T4C_PASSWORD=password \\\n  -e LOG_LEVEL=\"DEBUG\" \\\n  -e INCLUDE_COMMIT_HISTORY=false \\\n  t4c/client/base backup\n</code></pre> <p>Set the following values for the corresponding keys:</p> <ul> <li><code>GIT_REPO_URL</code>: URL to the target Git repository where the model will be   pushed to. All URI-formats supported by the <code>git clone</code> command will work.   You can provide HTTP credentials via the <code>GIT_USERNAME</code> and <code>GIT_PASSWORD</code>   variables (see below).</li> <li><code>GIT_REPO_BRANCH</code>: branch of the Git repository.</li> <li><code>GIT_USERNAME</code>: Git username if the repository is access protected.</li> <li><code>GIT_PASSWORD</code>: Git password that is used during cloning from and pushing to   the Git repository.</li> <li><code>T4C_REPO_HOST</code>: hostname to the T4C server. The same value that you enter in   Capella to connect to a remote repository.</li> <li><code>T4C_REPO_PORT</code>: port to the T4C server. The same value that you enter in   Capella to connect to a remote repository. Defaults to 2036.</li> <li><code>T4C_REPO_NAME</code>: T4C repository name. The same value that you enter in   Capella to connect to a remote repository.</li> <li><code>T4C_PROJECT_NAME</code>: name of the Capella project. It's displayed in the   Capella project explorer and in the last step when connecting to a remote   repository.</li> <li><code>T4C_USERNAME</code>: T4C username that is used during the import. The user needs   to have access to the repository.</li> <li><code>T4C_PASSWORD</code>: T4C password that is used during the import.</li> <li><code>LOG_LEVEL</code>: your preferred logging level (all Python logging levels are   supported).</li> <li><code>INCLUDE_COMMIT_HISTORY</code>: <code>true</code> or <code>false</code> to define if the T4C commit   history should be exported. Important: Exporting the commit history can take   a few hours for large models.</li> </ul>"},{"location":"capella/t4c/importer/#extract-teamforcapella-commit-messages-to-git","title":"Extract TeamForCapella commit messages to Git","text":"<p>The importer extracts the commit messages from TeamForCapella and adds them to the Backup commit description. The commit has the format:</p> <pre><code>Backup\n\n- user: admin\n  time: '2024-03-25T16:51:27.697000+00:00'\n  description: ''\n- user: admin\n  time: '2024-03-25T16:51:20.523000+00:00'\n  description: null\n- user: admin\n  time: '2024-03-25T16:51:09.755000+00:00'\n  description: Second Example commit\n- user: admin\n  time: '2024-03-25T16:50:57.138000+00:00'\n  description: First example commit\n</code></pre> <p>The commit body is always in the YAML format.</p>"},{"location":"capella/t4c/importer/#testing","title":"Testing","text":""},{"location":"capella/t4c/importer/#manual-testing","title":"Manual Testing","text":"<p>For development purposes, you can test the importer / backup locally.</p> <p>Warning</p> <p>For the next steps, you need a running TeamForCapella server and a TeamForCapella license server.</p> <ol> <li> <p>Start a lightweight local Git server with the following command:    <pre><code>make run-local-git-server\n</code></pre></p> </li> <li> <p>Create a new TeamForCapella repository via the REST API.    If you want to include the user information in the commits, you have to enable authentication.</p> </li> <li> <p>Set the <code>T4C_REPO_NAME</code> environment variable to the repository name that you    chose in the previous step:</p> <pre><code>export T4C_REPO_NAME=\"repoCapella\"\n</code></pre> </li> <li> <p>Open Capella and create a new Capella project with the project name \"test\".</p> </li> <li>Export the project to the TeamForCapella repository</li> <li> <p>Run the following command to start the importer:</p> <pre><code>make run-t4c/client/backup\n</code></pre> </li> <li> <p>Clone the sample repository:</p> <pre><code>git clone 'http://localhost:10001/git/git-test-repo.git'\n</code></pre> </li> <li> <p>Check the commit history and the model in the repository.</p> </li> </ol>"},{"location":"capella/t4c/introduction/","title":"Introduction","text":""},{"location":"capella/t4c/introduction/#teamforcapella-client-introduction","title":"TeamForCapella client introduction","text":"<p>Info</p> <p>TeamForCapella is a commercial product of OBEO. More information: https://www.obeosoft.com/en/team-for-capella</p>"},{"location":"ci-templates/","title":"Index","text":""},{"location":"ci-templates/#cicd-templates","title":"CI/CD templates","text":""},{"location":"ci-templates/#gitlab-cicd","title":"Gitlab CI/CD","text":"<p>Currently, we provide the following Gitlab CI/CD templates:</p> <ul> <li>Export to T4C: Export model in repository to T4C   using the merge strategy</li> <li>Diagram cache: Export diagrams of a Capella   model and store them in Gitlab artifacts</li> <li>Image builder: Build and push all Docker images to any   Docker registry.</li> <li>Model validation: Runs the Capella model validation CLI   tool.</li> </ul> <p>And the following Github CI/CD actions:</p> <ul> <li>Diagram cache: Export diagrams of a Capella   model and store them in Github artifacts, with the option to push it to a new   branch</li> </ul> <p>We offer more templates as part of our <code>py-capellambse</code> project. Please have a look here.</p>"},{"location":"ci-templates/github/diagram-cache/","title":"Diagram cache","text":""},{"location":"ci-templates/github/diagram-cache/#diagram-cache","title":"Diagram cache","text":"<p>Please add the following section to your <code>.github/workflows/diagram-cache.yml</code>:</p> <pre><code>name: update_capella_diagram_cache\n\non: push\n\njobs:\n  update_capella_diagram_cache:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v3\n      - name: Generate and upload diagram cache\n        uses: DSD-DBS/capella-dockerimages/ci-templates/github/diagram-cache@main\n        with:\n          entry_point: test/test.aird # Relative entrypoint to .aird file inside repository (starting from the root of the repository).\n</code></pre> <p>This is the minimal configuration, specifying only the <code>entry_point</code>. Nevertheless, there are several other options to configure your diagram cache workflow. For more options please have a look at the possible inputs</p>"},{"location":"ci-templates/gitlab/diagram-cache/","title":"Diagram cache","text":""},{"location":"ci-templates/gitlab/diagram-cache/#diagram-cache","title":"Diagram cache","text":"<p>Please add the following section to your <code>.gitlab-ci.yml</code>:</p> <pre><code>variables:\n  CAPELLA_VERSION: 6.0.0 # Enter the Capella version of the model here\n\ninclude:\n  - remote: https://raw.githubusercontent.com/DSD-DBS/capella-dockerimages/${CAPELLA_DOCKER_IMAGES_REVISION}/ci-templates/gitlab/diagram-cache.yml\n\nupdate_capella_diagram_cache:\n  variables:\n    ENTRY_POINT: test/test.aird # Entry point to the .aird file of the model (relative from root level of the repository)\n</code></pre> <p>In addition, you have to add the following environment variables on repository level. Make sure to enable the \"Expand variable reference\" flag.</p> <ul> <li><code>CAPELLA_DOCKER_IMAGES_REVISION</code>: Revision of this Github repository</li> </ul> <p>This is the minimal configuration. For more advanced configuration options, please refer to the Gitlab CI template.</p>"},{"location":"ci-templates/gitlab/image-builder/","title":"Image builder","text":""},{"location":"ci-templates/gitlab/image-builder/#image-builder","title":"Image builder","text":"<p>The image builder template builds all images supported by this repository, and pushes them to any Docker registry. We use it in our automated deployment environment for our Collaboration project. We have restricted internet access in our build environment, so the Gitlab CI template is optimized for restricted network access.</p> <p>Please add the following section to your <code>.gitlab-ci.yml</code>:</p> <pre><code>include:\n  - remote: https://raw.githubusercontent.com/DSD-DBS/capella-dockerimages/${CAPELLA_DOCKER_IMAGES_REVISION}/ci-templates/gitlab/image-builder.yml\n</code></pre>"},{"location":"ci-templates/gitlab/image-builder/#tagging-of-images","title":"Tagging of images","text":""},{"location":"ci-templates/gitlab/image-builder/#capella","title":"Capella","text":"<p>The resulting Capella based images will be tagged in the following format: <code>$CAPELLA_VERSION-$CAPELLA_DOCKER_IMAGES_REVISION-$GITLAB_IMAGE_BUILDER_REVISION</code>, e.g., <code>6.0.0-v1.7.0-v1.0.0</code>.</p> <p>where:</p> <ul> <li><code>$CAPELLA_VERSION</code> is the semantic Capella version, e.g., <code>6.0.0</code> or <code>5.2.0</code></li> <li> <p><code>$CAPELLA_DOCKER_IMAGES_REVISION</code> is the revision of this Github repository.</p> </li> <li> <p>Any branch or tag name is supported as revision</p> </li> <li> <p>All characters matching the regex <code>[^a-zA-Z0-9.]</code> will be replaces with <code>-</code></p> </li> <li> <p><code>$GITLAB_IMAGE_BUILDER_REVISION</code> is the revision of the Gitlab repository,   where the Gitlab CI template is included.</p> </li> <li> <p>We use the     predefined Gitlab CI variable <code>$CI_COMMIT_REF_NAME</code> to determine the name of the branch or tag.</p> </li> <li>This part can be used for your own versioning, e.g., when you have to patch     the Capella archives, but the semantic version is still the same.</li> </ul>"},{"location":"ci-templates/gitlab/image-builder/#papyrus","title":"Papyrus","text":"<p>The resulting Papyrus based images will be tagged in the following format: <code>$PAPYRUS_VERSION-$CAPELLA_DOCKER_IMAGES_REVISION-$GITLAB_IMAGE_BUILDER_REVISION</code>, e.g., <code>6.4.0-v1.7.0-v1.0.0</code>.</p>"},{"location":"ci-templates/gitlab/image-builder/#variables","title":"Variables","text":"<p>In addition, you have to add the following environment variables on repository level. Make sure to enable the \"Expand variable reference\" flag.</p> <ul> <li><code>CAPELLA_DOCKER_IMAGES_REVISION</code>: Revision of this Github repository.</li> <li><code>ENVIRONMENT</code>: Specifies the environment. In addition, you need to have the   following variables for each environment:</li> <li><code>UID_${ENVIRONMENT}</code>: The user ID which will be used for the technical     user.</li> <li>Variables related to the Docker registry (all parameters are passed to     <code>docker login</code>):<ul> <li><code>DOCKER_REGISTRY_${ENVIRONMENT}</code>: The URL to the Docker registry</li> <li><code>DOCKER_REGISTRY_USER_${ENVIRONMENT}</code>: Username of a techuser with push   permission to the Docker registry</li> <li><code>DOCKER_REGISTRY_PASSWORD_${ENVIRONMENT}</code>: Corresponding password of the   techuser</li> </ul> </li> </ul>"},{"location":"ci-templates/gitlab/image-builder/#repository-tree","title":"Repository tree","text":"<p>The tree inside of your Gitlab repository should look like:</p> <pre><code>\u251c\u2500\u2500 capella\n\u2502   \u2514\u2500\u2500 versions\n\u2502       \u251c\u2500\u2500 6.1.0\n\u2502       \u2502   \u251c\u2500\u2500 capella.tar.gz\n\u2502       \u2502   \u251c\u2500\u2500 dropins\n\u2502       \u2502   \u2514\u2500\u2500 updateSite\n\u2502       \u2514\u2500\u2500 7.0.0\n\u2502           \u251c\u2500\u2500 capella.tar.gz\n\u2502           \u251c\u2500\u2500 dropins\n\u2502           \u2514\u2500\u2500 updateSite\n\u251c\u2500\u2500 papyrus\n\u2502   \u2514\u2500\u2500 versions\n\u2502       \u2514\u2500\u2500 6.4.0\n\u2502           \u2514\u2500\u2500 papyrus.tar.gz\n\u2514\u2500\u2500 pure-variants\n    \u251c\u2500\u2500 dependencies\n    \u2514\u2500\u2500 updateSite\n</code></pre> <p>This is the minimal configuration. For more advanced configuration options, please refer to the Gitlab CI template.</p>"},{"location":"ci-templates/gitlab/model-validation/","title":"Model validation","text":""},{"location":"ci-templates/gitlab/model-validation/#model-validation","title":"Model validation","text":"<p>Currently, the model validation does NOT support:</p> <ul> <li>Loading of Capella libraries (only Capella projects without libraries are   supported)</li> <li>Usage of subdirectories with the <code>$ENTRYPOINT</code>. The <code>.aird</code>-file has to be   located in the root directory of the repository.</li> <li>Projects without <code>.project</code>-file. The <code>.project</code> has to be located in the   root directory of the repository.</li> </ul> <p>Please add the following section to your <code>.gitlab-ci.yml</code>:</p> <pre><code>variables:\n  CAPELLA_VERSION: 6.0.0 # Enter the Capella version of the model here, only versions &gt;= 6.0.0 are supported\n  ENTRYPOINT: test.aird # Filename of the `.aird` file\n\ninclude:\n  - remote: https://raw.githubusercontent.com/DSD-DBS/capella-dockerimages/${CAPELLA_DOCKER_IMAGES_REVISION}/ci-templates/gitlab/model-validation.yml\n</code></pre> <p>For more information, please refer to the Gitlab CI template.</p>"},{"location":"ci-templates/gitlab/release-train/","title":"Release train","text":""},{"location":"ci-templates/gitlab/release-train/#release-train","title":"Release train","text":"<p>The image builder Gitlab CI/CD template is limited to one environment and one Capella version. In addition, we provide a release train template, which can be used to trigger the image builder pipeline with a matrix of Capella versions and environments.</p> <p>Warning</p> <p>To continue, create a new image builder Gitlab repository and follow the instuctions of the image builder template.</p> <p>The pipeline is not triggered automatically. There are a few options to trigger the pipeline automatically:</p> <p>For changes on Github in this repository (e.g., new push to branches or a new tag):</p> <ul> <li>If you're using the PREMIUM version of Gitlab, pipelines can be automatically   triggered for external repositories:   https://docs.gitlab.com/ee/ci/ci_cd_for_external_repos/</li> <li>Detect changes on the remote with <code>git ls-remote</code> and trigger a Gitlab CI   pipeline with a cronjob.</li> </ul> <p>For changes in your Gitlab image builder repository:</p> <ul> <li>Use a Gitlab trigger to   trigger the release train pipeline.</li> </ul> <p>You can customize the pipeline to your needs (e.g., define multiple environments). The template provides three different jobs <code>.base</code>, <code>.capella</code> and <code>.jupyter</code> which can be extended:</p> <pre><code>include:\n  - remote: https://raw.githubusercontent.com/DSD-DBS/capella-dockerimages/${CAPELLA_DOCKER_IMAGES_REVISION}/ci-templates/gitlab/release-train.yml\"\n\n.staging: &amp;staging\n  rules:\n    # For commits on the main branch, build for the staging environment\n    - if: '$CI_COMMIT_REF_NAME == \"main\"'\n    - when: manual\n  variables:\n    ENVIRONMENT: staging\n    CAPELLA_DOCKER_IMAGES_REVISION: \"$CI_COMMIT_REF_NAME\"\n    IMAGE_BUILDER_GITLAB_REPOSITORY: \"$IMAGE_BUILDER_GITLAB_REPOSITORY\"\n    BUILD_FOR_LATEST_TAG: \"0\"\n\n.production: &amp;production\n  rules:\n    # For tags, build for the production environment\n    - if: \"$CI_COMMIT_TAG != null\"\n  variables:\n    ENVIRONMENT: production\n    CAPELLA_DOCKER_IMAGES_REVISION: \"$CI_COMMIT_REF_NAME\"\n    IMAGE_BUILDER_GITLAB_REPOSITORY: \"$IMAGE_BUILDER_GITLAB_REPOSITORY\"\n    BUILD_FOR_LATEST_TAG: \"1\"\n\nproduction-base:\n  extends: .base\n  &lt;&lt;: *production\n\nproduction-jupyter:\n  extends: .jupyter\n  needs:\n    - job: production-base\n      optional: true\n  &lt;&lt;: *production\n\nproduction-capella:\n  extends: .capella\n  needs:\n    - job: production-base\n      optional: true\n  &lt;&lt;: *production\n\nstaging-base:\n  extends: .base\n  &lt;&lt;: *staging\n\nstaging-jupyter:\n  extends: .jupyter\n  needs:\n    - job: staging-base\n      optional: true\n  &lt;&lt;: *staging\n\nstaging-capella:\n  extends: .capella\n  needs:\n    - job: staging-base\n      optional: true\n  &lt;&lt;: *staging\n</code></pre>"},{"location":"ci-templates/gitlab/t4c-export/","title":"T4C export","text":""},{"location":"ci-templates/gitlab/t4c-export/#export-to-t4c","title":"Export to T4C","text":"<p>Please add the following section to your <code>.gitlab-ci.yml</code>:</p> <pre><code>variables:\n  CAPELLA_VERSION: 6.0.0 # Enter the Capella version of the model here, only versions &gt;= 6.0.0 are supported\n\ninclude:\n  - remote: https://raw.githubusercontent.com/DSD-DBS/capella-dockerimages/${CAPELLA_DOCKER_IMAGES_REVISION}/ci-templates/gitlab/exporter.yml\n\nexport-to-t4c:\n  variables:\n    T4C_REPO_HOST: localhost # Hostname of the T4C server\n    T4C_PROJECT_NAME: example # Project name in the T4C repository\n    T4C_REPO_NAME: testrepo # T4C repository name\n</code></pre> <p>In addition, you have to add the following environment variables on repository level. Make sure to enable the \"Expand variable reference\" flag.</p> <ul> <li><code>CAPELLA_DOCKER_IMAGES_REVISION</code>: Revision of this Github repository</li> <li><code>T4C_USERNAME</code> and <code>T4C_PASSWORD</code>: Username / password for the T4C repository</li> </ul> <p>This is the minimal configuration. For more advanced configuration options, please refer to the Gitlab CI template.</p>"},{"location":"eclipse/base/","title":"Base","text":""},{"location":"eclipse/base/#eclipse-base","title":"Eclipse base","text":"<p>Info</p> <p>The Docker image name for this image is <code>eclipse/base</code></p> <p>The Eclipse base image runs the Eclipse Project in a Docker container. The Eclipse client can be downloaded and can optionally be customised prior to building the Docker image. EGit is installed as default package.</p>"},{"location":"eclipse/base/#supported-versions","title":"Supported versions","text":"<p>The image has been tested with the following versions:</p> <ul> <li>4.26.0</li> <li>4.27.0</li> </ul>"},{"location":"eclipse/base/#supported-architectures","title":"Supported architectures","text":"<p>We support and have tested the image against the <code>arm64</code> and <code>amd64</code> build architectures for the supported versions.</p>"},{"location":"eclipse/base/#use-the-prebuilt-image","title":"Use the prebuilt image","text":"<p>The Eclipse image is not available as prebuilt image yet.</p>"},{"location":"eclipse/base/#build-it-yourself","title":"Build it yourself","text":""},{"location":"eclipse/base/#preparation","title":"Preparation","text":""},{"location":"eclipse/base/#download-eclipse","title":"Download Eclipse","text":"<p>Download a Eclipse Linux binary <code>tar.gz</code> archive. You can get a release directly from Eclipse. Visit https://download.eclipse.org/eclipse/downloads/, then find a release in the <code>Latest Release</code> section. Scroll down to \"Platform Runtime Binary\" and select the package for the Linux platform with the matching build architecture.</p> <p>Place the downloaded archive in the subdirectory <code>eclipse/versions/$ECLIPSE_VERSION/$BUILD_ARCHITECTURE</code> of the present repository and ensure that the end result is either</p> <ul> <li><code>eclipse/versions/$ECLIPSE_VERSION/$BUILD_ARCHITECTURE/eclipse.tar.gz</code>.</li> </ul> <p>where <code>ECLIPSE_VERSION</code> refers to the semantic version of Eclipse, e.g. <code>4.27.0</code>.</p>"},{"location":"eclipse/base/#optional-customisation-of-the-eclipse-client","title":"Optional: Customisation of the Eclipse client","text":"<p>To customise the Eclipse client you can</p> <ol> <li>extract the downloaded archive,</li> <li>apply any modifications (e.g., installation of plugins and/ or dropins) to    it, and</li> <li>compress the modified folder to get a <code>eclipse.tar.gz</code> again.</li> </ol>"},{"location":"eclipse/base/#build-it-manually-with-docker","title":"Build it manually with Docker","text":"<pre><code>docker build -t eclipse/base eclipse --build-arg ECLIPSE_VERSION=$ECLIPSE_VERSION\n</code></pre>"},{"location":"eclipse/base/#run-the-container","title":"Run the container","text":""},{"location":"eclipse/base/#locally-on-x11-systems","title":"Locally on X11 systems","text":"<p>If you don't need remote access, have a local X11 server running and just want to run Eclipse locally, this may be the best option for you.</p> <p>On some systems, you have to whitelist connections to the X-Server with:</p> <pre><code>xhost +local\n</code></pre> <p>It allows all local programs to connect to your X server. You can further restrict the access to the X server. Please read the documentation of <code>xhost</code> for more details.</p> <p>The container can be started with the following command. The <code>DISPLAY</code> environment has to be passed to the container.</p> <pre><code>docker run -d \\\n    -v /tmp/.X11-unix:/tmp/.X11-unix \\\n    -e DISPLAY=$(DISPLAY) \\\n    eclipse/base\n</code></pre> <p>Eclipse should start after a few seconds.</p>"},{"location":"eclipse/base/#in-a-remote-container-rdp","title":"In a remote container (RDP)","text":"<p>Please follow the instructions on the remote page. When running the image, add the following variables to the <code>docker run</code> command:</p> <pre><code>    -e AUTOSTART_ECLIPSE=$AUTOSTART_ECLIPSE \\\n    -e RESTART_ECLIPSE=$RESTART_ECLIPSE \\\n</code></pre> <p>Please replace the followings variables:</p> <ul> <li><code>AUTOSTART_ECLIPSE</code> defines the autostart behaviour of Eclipse. When set to 1   (default), Eclipse will be started as soon as an RDP connection has been   established to the running container.</li> <li><code>RESTART_ECLIPSE</code> defines the restart behaviour of Eclipse. When set to 1   (default) and when <code>RESTART_ECLIPSE=1</code>, Eclipse will be re-started as soon as   it has been exited (after clean quits as well as crashs).</li> </ul> <p>If you want to configure the JVM memory options, have a look at Eclipse memory options.</p>"},{"location":"eclipse/memory-options/","title":"Memory Options","text":""},{"location":"eclipse/memory-options/#memory-options-for-eclipse","title":"Memory options for Eclipse","text":"<p>To specify fixed memory options for the JVM, you can pass the following environment variables to the <code>docker run</code> commands:</p> <ul> <li><code>MEMORY_MIN</code> (default <code>70%</code>), translated to <code>-Xms</code> for absolute values and   <code>-XX:InitialRAMPercentage</code> and <code>-XX:MinRAMPercentage</code> for percentage values.   Percentage values are calculated according to the total memory or the   requested memory by the container.</li> <li><code>MEMORY_MAX</code> (default <code>90%</code>), translated to <code>-Xmx</code> for absolute values and   <code>-XX:MaxRAMPercentage</code> for percentage values. Percentage values are   calculated according to the total memory of the system or the total memory   available to the container.</li> </ul> <p>If the value ends with a <code>%</code>, we assume that it's a percentage value.</p> <ul> <li>If used in a Kubernetes cluster, it determines the values from the Pod   requests/limits.</li> <li>If used on a host system, it determines the value from the host memory.</li> </ul> <p>See also:</p> <ul> <li>https://stackoverflow.com/a/65327769</li> <li>https://www.merikan.com/2019/04/jvm-in-a-container/#java-10</li> </ul>"},{"location":"git-hooks/git-hooks/","title":"Git hooks (pre-commit)","text":""},{"location":"git-hooks/git-hooks/#support-for-git-hooks","title":"Support for Git Hooks","text":"<p>To support Git hooks, we have decided to utilize the pre-commit framework. This framework allows for automatic fetching and updating of hooks from external sources, providing users with a high level of flexibility and adaptability. Additionally, it effectively decouples the development process of Git hooks from our Docker image release lifecycle.</p> <p>The implementation was tested with the official Git CLI and JGit/EGit (in Eclipse).</p>"},{"location":"git-hooks/git-hooks/#project-specific-hooks","title":"Project-Specific Hooks","text":"<p>Project-specific hooks can be defined separately for each Git repository. The defined Git hooks will not be auto-updated by us. Instead, the responsibility falls on the repository owners to keep them up-to-date and maintain them.</p>"},{"location":"git-hooks/git-hooks/#installation","title":"Installation","text":"<p>To install Git hooks, add a <code>.pre-commit-config.yaml</code> file to the root of your repository. Simply follow the official pre-commit instructions: Add a pre-commit configuration. Commit the file after making changes. Git will automatically detect it, eliminating the need to explicitly install the hooks via <code>pre-commit install</code>.</p> <p>Info</p> <p><code>pre-commit</code> is installed for all stages by default. This means that your pre-commit hooks will run at every stage. If you want to limit the hooks to specific stages, for example, to run hooks only at the <code>pre-commit</code> stage, add the following to your <code>.pre-commit-config.yaml</code> file:</p> <pre><code>default_stages: [pre-commit]\n</code></pre> <p>Alternatively, specify stages for each hook individually:</p> <pre><code>- repo: local\n  hooks:\n    - id: my-hook\n      stages: [pre-commit]\n</code></pre> <p>For more information, refer to the <code>pre-commit</code> documentation: Confining hooks to run at certain stages.</p>"},{"location":"git-hooks/git-hooks/#updates","title":"Updates","text":"<p>To auto-update Git hooks, update the versions in your repository's <code>.pre-commit-config.yaml</code> file. Refer to the official documentation for updating hooks automatically.</p>"},{"location":"git-hooks/git-hooks/#technical-background","title":"Technical Background","text":"<p>To facilitate this implementation, we have globally set the <code>core.hooksPath</code> configuration option in Git to <code>/opt/git/global-hooks</code>. We initialize the pre-commit framework in this directory. This ensures that the hook is registered as <code>/opt/git/global-hooks/pre-commit</code>. Since the initialization of Git hooks with the pre-commit framework can be slow, we maintain a cache of the pre-commit environment and store it in the persistent <code>/workspace</code> directory.</p>"},{"location":"git-hooks/git-hooks/#troubleshooting","title":"Troubleshooting","text":"<p>Error Message in Eclipse with EGit</p> <p>If you receive an error message in Eclipse (as shown below) while committing, your pre-commit has failed. </p> <p>Please check the Capella logs, located at <code>/var/log/session</code>.</p>"},{"location":"jupyter/","title":"Jupyter Notebooks","text":""},{"location":"jupyter/#build-the-jupyter-notebook-image","title":"Build the <code>jupyter-notebook</code> image","text":"<p>The <code>jupyter-notebook</code> image provides a JupyterLab server that can run on the Collab-Manager environment.</p> <p>The image is configured to connect to the same workspace shared volume as the Capella remote images. If the <code>notebooks/</code> folder on the shared volume contains a <code>requirements.txt</code> file, dependencies defined in that file will be installed before the server launches.</p> <pre><code>docker build -t jupyter-notebook jupyter-notebook\n</code></pre>"},{"location":"jupyter/#run-the-jupyter-notebook-image","title":"Run the <code>jupyter-notebook</code> image","text":"<pre><code>docker run -ti --rm -e WORKSPACE_DIR=/tmp/notebooks -p 8888:8888 jupyter-notebook\n</code></pre> <p>The following environment variables can be defined:</p> <ul> <li><code>JUPYTER_PORT</code>: The port to run the jupyter server on.</li> <li><code>WORKSPACE_DIR</code>: The working directory for JupyterLab.</li> <li><code>JUPYTER_BASE_URL</code>: A context path to access the jupyter server. This allows   you to run multiple server containers on the same domain.</li> <li><code>JUPYTER_ADDITIONAL_DEPENDENCIES</code>: A space-separated list of additional pip   dependencies to install. The variable is passed to the <code>pip install -U</code>   command and may include additional flags. The value is not escaped, only use   trusted values.</li> </ul>"},{"location":"papyrus/base/","title":"Base","text":""},{"location":"papyrus/base/#papyrus-base","title":"Papyrus base","text":"<p>Info</p> <p>The Docker image name for this image is <code>papyrus/base</code></p> <p>The Papyrus base image runs Eclipse Payprus in a Docker container. The Papyrus client can be downloaded and can optionally be customised prior to building the Docker image.</p>"},{"location":"papyrus/base/#supported-versions","title":"Supported versions","text":"<p>The image has been tested with the following versions:</p> <ul> <li>6.4.0 (2023-03 release)</li> </ul> <p>The only supported build architecture is amd64. To build and run the image on other build architectures, use QEMU or Rosetta.</p>"},{"location":"papyrus/base/#use-the-prebuilt-image","title":"Use the prebuilt image","text":"<p>The Papyrus image is not available as prebuilt image yet.</p>"},{"location":"papyrus/base/#build-it-yourself","title":"Build it yourself","text":""},{"location":"papyrus/base/#preparation","title":"Preparation","text":""},{"location":"papyrus/base/#download-papyrus","title":"Download Papyrus","text":"<p>Download a Papyrus Linux binary <code>tar.gz</code> archive. You can get a release directly from Papyrus. Visit https://www.eclipse.org/papyrus/download.html to find the \"Latest Released RCP\".</p> <p>Place the downloaded archive in the subdirectory <code>papyrus/versions/$PAPYRUS_VERSION</code> of the present repository and ensure that the end result is either</p> <ul> <li><code>papyrus/versions/$PAPYRUS_VERSION/papyrus.tar.gz</code>.</li> </ul> <p>where <code>PAPYRUS_VERSION</code> refers to the semantic version of Papyrus, e.g. <code>6.4.0</code> for the 2023-03 release.</p>"},{"location":"papyrus/base/#optional-customisation-of-the-papyrus-client","title":"Optional: Customisation of the Papyrus client","text":"<p>To customise the Papyrus client you can</p> <ol> <li>extract the downloaded archive,</li> <li>apply any modifications (e.g., installation of plugins and/ or dropins) to    it, and</li> <li>compress the modified folder to get a <code>papyrus.tar.gz</code> again.</li> </ol>"},{"location":"papyrus/base/#build-it-manually-with-docker","title":"Build it manually with Docker","text":"<pre><code>docker build -t papyrus/base papyrus --build-arg PAPYRUS_VERSION=$PAPYRUS_VERSION\n</code></pre> <p>If you want to configure the JVM memory options, have a look at Eclipse memory options.</p>"},{"location":"papyrus/base/#run-the-container","title":"Run the container","text":""},{"location":"papyrus/base/#locally-on-x11-systems","title":"Locally on X11 systems","text":"<p>If you don't need remote access, have a local X11 server running and just want to run Papyrus locally, this may be the best option for you.</p> <p>On some systems, you have to whitelist connections to the X-Server with:</p> <pre><code>xhost +local\n</code></pre> <p>It allows all local programs to connect to your X server. You can further restrict the access to the X server. Please read the documentation of <code>xhost</code> for more details.</p> <p>The container can be started with the following command. The <code>DISPLAY</code> environment has to be passed to the container.</p> <pre><code>docker run -d \\\n    -v /tmp/.X11-unix:/tmp/.X11-unix \\\n    -e DISPLAY=$(DISPLAY) \\\n    papyrus/base\n</code></pre> <p>Papyrus should start after a few seconds.</p>"},{"location":"papyrus/base/#in-a-remote-container-rdp","title":"In a remote container (RDP)","text":"<p>Please follow the instructions on the remote page. When running the image, add the following variables to the <code>docker run</code> command:</p> <pre><code>    -e AUTOSTART_PAPYRUS=$AUTOSTART_PAPYRUS \\\n    -e RESTART_PAPYRUS=$RESTART_PAPYRUS \\\n</code></pre> <p>Please replace the followings variables:</p> <ul> <li><code>AUTOSTART_PAPYRUS</code> defines the autostart behaviour of Papyrus. When set to 1   (default), Papyrus will be started as soon as an RDP connection has been   established to the running container.</li> <li><code>RESTART_PAPYRUS</code> defines the restart behaviour of Papyrus. When set to 1   (default) and when <code>RESTART_PAPYRUS=1</code>, Papyrus will be re-started as soon as   it has been exited (after clean quits as well as crashs).</li> </ul>"}]}